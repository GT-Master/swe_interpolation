{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gdal import gdalconst\n",
    "import osgeo.osr as osr\n",
    "from sklearn import mixture\n",
    "from sklearn.neighbors import KDTree, BallTree, DistanceMetric\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern, ExpSineSquared, DotProduct\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import pearsonr, linregress\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib.dates import MO\n",
    "from postgisUtil import postgisUtil\n",
    "import tensorflow as tf\n",
    "months = mdates.MonthLocator()\n",
    "monthsFmt = mdates.DateFormatter('%b-%Y')\n",
    "mpl.rc(\"font\", family=\"Helvetica\")\n",
    "mpl.rc('font', size=12)\n",
    "\n",
    "# ASO dates for both regions at different years, 2016 data needs to be downloaded\n",
    "date_list = {2014:{\"Merced\":[date(2014, 3, 24), \n",
    "                            date(2014, 4, 6), \n",
    "                             date(2014, 4, 14), \n",
    "                             date(2014, 4, 23), \n",
    "                             date(2014, 4, 29), \n",
    "                             date(2014, 5, 3), \n",
    "                             date(2014, 5, 12)], \n",
    "                  \"Tuolumne\": [date(2014, 3, 23), \n",
    "                             date(2014, 4, 7), \n",
    "                             date(2014, 4, 13), \n",
    "                             date(2014, 4, 20), \n",
    "                             date(2014, 4, 28), \n",
    "                             date(2014, 5, 2), \n",
    "                             date(2014, 5, 11)]},\n",
    "             2016: {\"Tuolumne\": [date(2016, 4, 1), \n",
    "                                 date(2016, 4, 7),\n",
    "                                 date(2016, 4, 16),\n",
    "                                 date(2016, 4, 26),\n",
    "                                 date(2016, 5, 27),\n",
    "                                 date(2016, 6, 7), \n",
    "                                 date(2016, 6, 13),\n",
    "                                 date(2016, 6, 20)]}}\n",
    "\n",
    "# Some predefined parameters, for both regions\n",
    "density_factor = {\"Merced\": 0.333, \"Tuolumne\": 1.0}\n",
    "site_name_abbr = {\"Merced\": \"mb\", \"Tuolumne\":\"tb\"}\n",
    "\n",
    "# Connect to the postgresql db\n",
    "kNN_db = postgisUtil(database='knn_project', username='zeshi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print BallTree.valid_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing the data, convert all historical reconstruction data into the same shape as 500-m Lidar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is used for loading the features \n",
    "def load_features(sensor_feature=False, exclude_null=True, basin='Merced'):\n",
    "    # Please note that the indices here are indices of python, starting from 0\n",
    "    features = ['DEM', 'SLP', 'ASP', 'VEG']\n",
    "    if not sensor_feature:\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_array = kNN_db.query_map(feature, 'topo', basin.lower())\n",
    "            if i == 0:\n",
    "                grid_y, grid_x = np.meshgrid(range(feature_array.shape[0]), range(feature_array.shape[1]), indexing='ij')\n",
    "                grid_y_array, grid_x_array = grid_y.flatten(), grid_x.flatten()\n",
    "                feature_space = np.column_stack((grid_y_array, np.column_stack((grid_x_array, feature_array.flatten()))))\n",
    "            else:\n",
    "                feature_space = np.column_stack((feature_space, feature_array.flatten()))\n",
    "        if exclude_null:\n",
    "            feature_space = feature_space[feature_space[:, 2] >= 0]\n",
    "        feature_space[feature_space < 0] = np.nan\n",
    "    else:\n",
    "        feature_array = kNN_db.geoms_table_to_map_pixel_values(features, 'sensors', 'site_locs', 'site_coords', 'topo', basin.lower())\n",
    "        spatial_feature = kNN_db.geoms_table_to_map_pixel_indices(features[0], 'sensors', 'site_locs', 'site_coords', 'topo', basin.lower())\n",
    "        feature_space = np.column_stack((spatial_feature, feature_array))\n",
    "    return feature_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to find representative 20-ish pixels of the basin using either k-means or Gaussian-mixture model. And use them to run kNN's in the real-time interpolation scheme\n",
    "\n",
    "GMM tuning, the Gaussian mixture model needs to include the following features\n",
    "$$\\mathrm{X}=[x_{lat}, x_{lon}, x_{elev}, x_{slope}, x_{aspect}, x_{canopy}]$$\n",
    "But according to the result we could minimize the error in the results by further add a few more networks in the higher elevation zones and doing the estimation separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the CDF of the selected pixels and the entire basin's features. If they match up nicely, it means that the selected pixels are representative of the entire basin's physiographic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cdf_compare(sensor_loc, site_name='Merced', feature_str='DEM'):\n",
    "    # Load topo features form DB\n",
    "    dem_array = kNN_db.query_map(\"DEM\", \"topo\", site_name)\n",
    "    feature_array = kNN_db.query_map(feature_str, \"topo\", site_name)\n",
    "    \n",
    "    # Indexing sensor features\n",
    "    sensor_feature = feature_array[sensor_loc]\n",
    "    \n",
    "    # Filter feature to specific dem bands\n",
    "    feature = feature_array[dem_array >= 1500.]\n",
    "    \n",
    "    # Generate histogram of sensors and basins\n",
    "    sensor_hist = np.histogram(sensor_feature, normed=True)\n",
    "    basin_hist = np.histogram(feature, bins=100, normed=True)\n",
    "    \n",
    "    # Generate CDF of sensors and basins\n",
    "    sensor_cdf = np.cumsum(sensor_hist[0]) / np.cumsum(sensor_hist[0])[-1]\n",
    "    sensor_cdf_feature = sensor_hist[1]\n",
    "    basin_cdf = np.cumsum(basin_hist[0]) / np.cumsum(basin_hist[0])[-1]\n",
    "    basin_cdf_feature = basin_hist[1]\n",
    "    sensor_cdf_feature = sensor_cdf_feature[:-1] + (sensor_cdf_feature[1] - sensor_cdf_feature[0]) / 2.\n",
    "    basin_cdf_feature = basin_cdf_feature[:-1] + (basin_cdf_feature[1] - basin_cdf_feature[0]) / 2.\n",
    "    \n",
    "    # Generate figure comparing the sensor and basin features\n",
    "    if feature_str == 'DEM':\n",
    "        sensor_cdf_feature = np.append([1500.], sensor_cdf_feature)\n",
    "        print sensor_feature\n",
    "    else:\n",
    "        sensor_cdf_feature = np.append([0.], sensor_cdf_feature)\n",
    "    sensor_cdf = np.append([0], sensor_cdf)\n",
    "    plt.step(sensor_cdf_feature, sensor_cdf, '-g')\n",
    "    plt.step(basin_cdf_feature, basin_cdf, '-b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get time series from historical reconstruction data, by using ```sensor_loc``` result from the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gdal\n",
    "from datetime import timedelta\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from matplotlib import pyplot as plt\n",
    "site_name_abbr = {\"Merced\": \"mb\", \"Tuolumne\":\"tb\"}\n",
    "density_factor = {\"Merced\": 0.333, \"Tuolumne\": 1.0}\n",
    "\n",
    "class kNN_swe_regressor():\n",
    "    def __init__(self, site_name, year, date_list, k):\n",
    "        self.site_name = site_name\n",
    "        self.year = year\n",
    "        self.date_list = date_list[self.year][site_name]\n",
    "        self.k = k\n",
    "        self.kNN = None\n",
    "    \n",
    "    # Construct kNN features for particular year and basin, only needs to run once\n",
    "    def kNN_feature_construct(self, sensor_loc):\n",
    "        recon_dir = site_name_abbr[self.site_name].upper() + \"_recon/\"\n",
    "        year_range = range(2001, 2014)\n",
    "        historical_fn_list = []\n",
    "        recon_ts = None\n",
    "        for year in year_range:\n",
    "            temp_date = date(year, 4, 1)\n",
    "            ending_date = date(year, 8, 31)\n",
    "            while temp_date <= ending_date:\n",
    "                temp_fn = recon_dir + temp_date.strftime(\"%d%b%Y\").upper() + \".tif\"\n",
    "                historical_fn_list.append(temp_fn)\n",
    "                temp_recon = gdal.Open(temp_fn).ReadAsArray()\n",
    "                temp_date += timedelta(days=1)\n",
    "                if recon_ts is None:\n",
    "                    recon_ts = temp_recon[sensor_loc]\n",
    "                else:\n",
    "                    recon_ts = np.vstack((recon_ts, temp_recon[sensor_loc]))\n",
    "        dump_fn = \"kNN_training_testing/library_\" + site_name_abbr[self.site_name].lower() + \"_2001_2013_filenames.p\"\n",
    "        pickle.dump(historical_fn_list, open(dump_fn, 'wb'))\n",
    "        np.save(\"kNN_training_testing/library_\" + site_name_abbr[self.site_name].lower() + \"_2001_2013.npy\", recon_ts)\n",
    "\n",
    "        sensor_data = None\n",
    "        for temp_date in self.date_list:\n",
    "            lidar_fn = \"ASO_Lidar/\" + site_name_abbr[self.site_name] + temp_date.strftime(\"%Y%m%d\") + \"_500m.tif\"\n",
    "            lidar_swe = gdal.Open(lidar_fn).ReadAsArray() * density_factor[self.site_name]\n",
    "            if sensor_data is None:\n",
    "                sensor_data = lidar_swe[sensor_loc]\n",
    "            else:\n",
    "                sensor_data = np.vstack((sensor_data, lidar_swe[sensor_loc]))\n",
    "        np.save(\"kNN_training_testing/\" + site_name_abbr[self.site_name] + \"_\" +str(self.year) + \\\n",
    "                \"_aso_simulated_sensor_data.npy\", sensor_data)\n",
    "    \n",
    "    # load k nearest neighbor training and testing data into memory\n",
    "    def load_kNN_data(self):\n",
    "        recon_ts = np.load(\"kNN_training_testing/library_\" + site_name_abbr[self.site_name].lower() + \"_2001_2013.npy\")\n",
    "        recon_fn = pickle.pickle.load(open(\"kNN_training_testing/library_\" + site_name_abbr[self.site_name].lower() + \\\n",
    "                                           \"mb_2001_2013_filenames.p\", \"rb\"))\n",
    "        lidar_sensor_data = np.load(\"kNN_training_testing/\" + site_name_abbr[self.site_name] + \"_\" +str(self.year) + \\\n",
    "                                    \"_aso_simulated_sensor_data.npy\")\n",
    "        emp_cov = EmpiricalCovariance().fit(recon_ts)\n",
    "        emp_cov_matrix = emp_cov.get_precision()\n",
    "        dist = DistanceMetric.get_metric('mahalanobis', V=emp_cov_matrix)\n",
    "        self.kNN = BallTree(recon_ts, metric=dist)\n",
    "        self.sensor = lidar_sensor_data\n",
    "    \n",
    "    # estimate the SWE for the 2D case for all days\n",
    "    def kNN_predict(self):\n",
    "        self.load_kNN_data()\n",
    "        self.est_tuple_list = []\n",
    "        dem = gdal.Open(\"ASO_Lidar/\" + self.site_name + \"_DEM_500m.tif\").ReadAsArray()\n",
    "        self.est_tuple_list = map(self.kNN_predict_mapper, zip(self.sensor, self.date_list))\n",
    "       \n",
    "    # Estimate SWE for each individual day\n",
    "    def kNN_predict_mapper(self, sensor_date_tuple):\n",
    "        sensor = sensor_date_tuple[0]\n",
    "        temp_date = sensor_date_tupe[1]\n",
    "        dist, idx = self.kNN.query(np.array([sensor]), k=k)\n",
    "        temp_fn_list = [recon_fn[i] for i in idx[0]]\n",
    "        kNN_map_sum = 0.\n",
    "        for temp_fn in temp_fn_list:\n",
    "            kNN_map_sum += gdal.Open(temp_fn).ReadAsArray()\n",
    "        kNN_map_avg = kNN_map_sum / float(k)\n",
    "        lidar_map = gdal.Open(\"ASO_Lidar/\"+site_name_abbr[self.site_name].upper() + \\\n",
    "                              temp_date.strftime(\"%Y%m%d\") + \"_500m.tif\").ReadAsArray() * density_factor[self.site_name]\n",
    "        recon_map = gdal.Open(site_name_abbr[self.site_name].upper() + \\\n",
    "                              \"_recon/\"+temp_date.strftime(\"%d%b%Y\").upper()+\".tif\").ReadAsArray()\n",
    "        snodas_map = gdal.Open(\"SNODAS/\" + site_name_abbr[self.site_name].upper() + \"_\" + \\\n",
    "                               temp_date.strftime(\"%Y%m%d\") + \".tif\").ReadAsArray() / 1000.\n",
    "        # Filter these data\n",
    "        kNN_map_avg_valid = kNN_map_avg[lidar_map >= 0.]\n",
    "        snodas_map_valid = snodas_map[lidar_map >= 0.]\n",
    "        recon_map_valid = recon_map[lidar_map >= 0.]\n",
    "        lidar_map_valid = lidar_map[lidar_map >= 0.]\n",
    "        elevation_map_valid = dem[lidar_map >= 0.]\n",
    "        \n",
    "        # return the tuple with all 2D maps flattened in 1D vector\n",
    "        return (kNN_map_avg_valid, recon_map_valid, snodas_map_valid, lidar_map_valid, elevation_map_valid)\n",
    "\n",
    "    # plot model results vs lidar\n",
    "    def kNN_recon_snodas_vs_lidar(self, snodas=True):\n",
    "        fig, axarr = plt.subplots(ncols=3, nrows=7, figsize=(6, 10))\n",
    "        self.kNN_lidar_slope = []\n",
    "        self.kNN_lidar_r = []\n",
    "        self.recon_lidar_slope = []\n",
    "        self.recon_lidar_r = []\n",
    "        for j, temp_date in enumerate(self.date_list):\n",
    "            print j\n",
    "\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(self.est_tuple_list[j][3], self.est_tuple_list[j][0])\n",
    "            x_0 = [0., 1.]\n",
    "            y_0 = [intercept, slope+intercept]\n",
    "            self.kNN_lidar_slope.append(slope)\n",
    "            self.kNN_lidar_r.append(r_value)\n",
    "            \n",
    "            slope, intercept, r_value, p_value, std_err = linregress(self.est_tuple_list[j][3], self.est_tuple_list[j][1])\n",
    "            x_1 = [0., 1.]\n",
    "            y_1 = [intercept, slope+intercept]\n",
    "            self.recon_lidar_slope.append(slope)\n",
    "            self.recon_lidar_r.append(r_value)\n",
    "\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(self.est_tuple_list[j][3], self.est_tuple_list[j][2])\n",
    "            x_2 = [0., 1.]\n",
    "            y_2 = [intercept, slope+intercept]\n",
    "\n",
    "            axarr[j, 0].plot(self.est_tuple_list[j][3], self.est_tuple_list[j][0], '.k', markersize=2)\n",
    "            axarr[j, 0].plot([0, 1], [0, 1], '--k')\n",
    "            axarr[j, 0].plot(x_0, y_0, '-b')\n",
    "            axarr[j, 0].set_xlim([0, 1])\n",
    "            axarr[j, 0].set_ylim([0, 1])\n",
    "            axarr[j, 1].plot(self.est_tuple_list[j][3], self.est_tuple_list[j][1], '.k', markersize=2)\n",
    "            axarr[j, 1].plot([0, 1], [0, 1], '--k')\n",
    "            axarr[j, 1].plot(x_1, y_1, '-b')\n",
    "            axarr[j, 1].set_xlim([0, 1])\n",
    "            axarr[j, 1].set_ylim([0, 1])\n",
    "            axarr[j, 2].plot(self.est_tuple_list[j][3], self.est_tuple_list[j][2], '.k', markersize=2)\n",
    "            axarr[j, 2].plot([0, 1], [0, 1], '--k')\n",
    "            axarr[j, 2].plot(x_2, y_2, '-b')\n",
    "            axarr[j, 2].set_xlim([0, 1])\n",
    "            axarr[j, 2].set_ylim([0, 1])\n",
    "            print np.min(self.est_tuple_list[j][3]]), np.max(self.est_tuple_list[j][3])\n",
    "            if j < len(self.date_list)-1:\n",
    "                axarr[j, 0].xaxis.set_ticklabels([])\n",
    "                axarr[j, 1].xaxis.set_ticklabels([])\n",
    "                axarr[j, 2].xaxis.set_ticklabels([])\n",
    "                axarr[j, 0].yaxis.set_ticklabels([\"\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"])\n",
    "            axarr[j, 1].yaxis.set_ticklabels([])\n",
    "            axarr[j, 2].yaxis.set_ticklabels([])\n",
    "            if j==len(self._date_list)-1:\n",
    "                axarr[j, 1].xaxis.set_ticklabels([\"\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"])\n",
    "                axarr[j, 2].xaxis.set_ticklabels([\"\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"])\n",
    "            axarr[j, 2].text(0.55, 0.1, temp_date.strftime(\"%Y-%m-%d\"), fontsize=8)\n",
    "        fig.text(0.35, 0.09, 'Lidar measured SWE, m')\n",
    "        fig.text(0.03, 0.6, 'kNN interpolated SWE, m', rotation=90)\n",
    "        fig.text(0.91, 0.6, 'SNODAS SWE, m', rotation=270)\n",
    "        plt.subplots_adjust(left=0.14, wspace=0, hspace=0)\n",
    "        plt.savefig(self.site_name + \"_simulated_kNN_recon_snodas_ASO.pdf\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Show scatterplot statistics of the\n",
    "    def scatter_statistics_figure(self):\n",
    "        fig, axarr = plt.subplots(ncols=1, nrows=2, figsize=(3.5, 4), sharex=True)\n",
    "        axarr[0].plot(self.date_list, self.kNN_lidar_slope, '-k')\n",
    "        axarr[0].plot(self.date_list, self.recon_lidar_slope, '--k')\n",
    "        axarr[0].set_ylabel(\"Slopes\")\n",
    "        axarr[0].set_ylim([0.6, 1.6])\n",
    "        axarr[0].yaxis.set_ticks([0.6, 0.8, 1., 1.2, 1.4, 1.6])\n",
    "        axarr[0].legend([\"kNN vs. Lidar\", \"Reconstruction vs. Lidar\"], fontsize=10, frameon=False, loc=4)\n",
    "        axarr[1].plot(date_list, self.kNN_lidar_r, '-k')\n",
    "        axarr[1].plot(date_list, self.recon_lidar_r, '--k')\n",
    "        axarr[1].xaxis.set_major_locator(months)\n",
    "        axarr[1].yaxis.set_ticks([0.55, 0.65, 0.75, 0.85])\n",
    "        axarr[1].set_ylabel(\"Correlation coefficients\")\n",
    "        axarr[1].xaxis.set_major_formatter(monthsFmt)\n",
    "        plt.savefig(\"slope_r_kNN_recon_Lidar.pdf\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def mean_variance_ts(self):\n",
    "        kNN_mean_list = []\n",
    "        lidar_mean_list = []\n",
    "        recon_mean_list = []\n",
    "        snodas_mean_list = []\n",
    "        \n",
    "        kNN_std_list = []\n",
    "        lidar_std_list = []\n",
    "        recon_std_list = []\n",
    "        snodas_std_list = []\n",
    "        \n",
    "        for j, temp_date in enumerate(date_list):\n",
    "            \n",
    "            kNN_mean = np.nanmean(self.est_tuple_list[j][0])\n",
    "            lidar_mean = np.nanmean(self.est_tuple_list[j][3])\n",
    "            recon_mean = np.nanmean(self.est_tuple_list[j][1])\n",
    "            snodas_mean = np.nanmean(self.est_tuple_list[j][2])\n",
    "            \n",
    "            kNN_std = np.nanstd(self.est_tuple_list[j][0])\n",
    "            lidar_std = np.nanstd(self.est_tuple_list[j][3])\n",
    "            recon_std = np.nanstd(self.est_tuple_list[j][1])\n",
    "            snodas_std = np.nanstd(self.est_tuple_list[j][2])\n",
    "            \n",
    "            kNN_mean_list.append(kNN_mean)\n",
    "            lidar_mean_list.append(lidar_mean)\n",
    "            recon_mean_list.append(recon_mean)\n",
    "            snodas_mean_list.append(snodas_mean)\n",
    "            \n",
    "            kNN_std_list.append(kNN_std)\n",
    "            lidar_std_list.append(lidar_std)\n",
    "            recon_std_list.append(recon_std)\n",
    "            snodas_std_list.append(snodas_std)\n",
    "            \n",
    "        kNN_mark = plt.errorbar(date_list, kNN_mean_list, kNN_std_list, linestyle='-', marker='o', color='r')\n",
    "        lidar_mark = plt.errorbar(date_list, lidar_mean_list, lidar_std_list, linestyle='--', marker='o', color='g')\n",
    "        recon_mark = plt.errorbar(date_list, recon_mean_list, recon_std_list, linestyle='-.', marker='o', color='b')\n",
    "        plt.xlim([min(self.date_list) - timedelta(days=5), max(self.date_list) + timedelta(days=5)])\n",
    "        plt.grid()\n",
    "        plt.ylim([0, max(max(kNN_mean_list), max(lidar_mean_list), max(recon_mean_list), max(snodas_mean_list)) + \\\n",
    "                  max(max(kNN_std_list), max(lidar_std_list), max(recon_std_list), max(snodas_std_list))])\n",
    "        \n",
    "        # Define date_locators separate x-axis by 14 days\n",
    "        date_locators = [min(self.date_list) + timedelta(days=dt) \n",
    "                         for dt in range(0, (max(self.date_list) - min(self.date_list)).days + 5, 14)]\n",
    "        \n",
    "        date_locators_string = [temp_date.strftime('%b %d %Y') for temp_date in date_locators]\n",
    "        plt.legend([kNN_mark, lidar_mark, recon_mark], ['kNN', 'Lidar', 'Reconstruction'], numpoints=1)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('SWE, m')\n",
    "        plt.xticks(date_locators, date_locators_string)\n",
    "        plt.savefig(self.site_name.upper() + \"_basin_wide_mean_std.pdf\", dpi=100)\n",
    "        plt.show()\n",
    "        \n",
    "    def elev_band_mean_std_comparison(self):\n",
    "        def elevation_avg(features):\n",
    "            elevation_gradient = np.linspace(1500, 3500, 40)\n",
    "            new_features = np.zeros((len(elevation_gradient)-1, 7))\n",
    "            for i, temp_elev in enumerate(elevation_gradient[:-1]):\n",
    "                min_elev = temp_elev\n",
    "                max_elev = elevation_gradient[i+1]\n",
    "                avg_elev = (min_elev + max_elev)/2.\n",
    "                temp_features = features[np.logical_and(features[:, 0] >= min_elev, features[:, 1] <= max_elev), :]\n",
    "                kNN_mean = np.nanmean(temp_features[:, -3])\n",
    "                kNN_std = np.nanstd(temp_features[:, -3])\n",
    "                lidar_mean = np.nanmean(temp_features[:, -2])\n",
    "                lidar_std = np.nanstd(temp_features[:, -2])\n",
    "                recon_mean = np.nanmean(temp_features[:, -1])\n",
    "                recon_std = np.nanstd(temp_features[:, -1])\n",
    "                new_features[i] = [avg_elev, kNN_mean, kNN_std, lidar_mean, lidar_std, recon_mean, recon_std]\n",
    "            return new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simulation using Lidar and reconstruction data. Please use BallTree data structure with ```mahalanobis``` distance metric for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The result of the figure showing above means that kNN could be able to estimate averaged snow water equivalent at each individual elevation bands correctly below 2500-m but could not estimate correctly when elevation is above 2500-m.\n",
    "\n",
    "A new strategy is forming up by separating the kNN estimations by two different elevation blocks <= 2500 and > 2500. sensor locations will be separted so as the estimation will at least match up at their elevation bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_k():\n",
    "    np.random.seed(1)\n",
    "    recon_ts = np.load(\"library_mb_2001_2013.npy\")\n",
    "    recon_fn = pickle.load(open(\"library_mb_2001_2013_filenames.p\", \"rb\"))\n",
    "    lidar_sensor_data = np.load(\"aso_simulated_sensor_data.npy\")\n",
    "    sensor = lidar_sensor_data[0]\n",
    "    temp_date = date_list[0]\n",
    "    percentage_list = np.arange(0.1, 1.01, 0.01) * 100.\n",
    "    k_list = []\n",
    "    for percentage_scenes in np.arange(0.1, 1.01, 0.01):\n",
    "        temp_recon_idx = np.random.randint(0, len(recon_ts), int(percentage_scenes * float(len(recon_ts))))\n",
    "        temp_recon_ts = recon_ts[temp_recon_idx]\n",
    "        temp_recon_fn = [recon_fn[i] for i in temp_recon_idx]\n",
    "        kNN = KDTree(temp_recon_ts)\n",
    "        rmse_k = []\n",
    "        for temp_k in range(1, 20):\n",
    "            dist, idx = kNN.query(np.array([sensor]), k=temp_k)\n",
    "            temp_fn_list = [temp_recon_fn[i] for i in idx[0]]\n",
    "            kNN_map_sum = 0.\n",
    "            for temp_fn in temp_fn_list:\n",
    "                kNN_map_sum += gdal.Open(temp_fn).ReadAsArray()\n",
    "            kNN_map_avg = kNN_map_sum / float(temp_k)\n",
    "            lidar_map = gdal.Open(\"ASO_Lidar/MB\"+temp_date.strftime(\"%Y%m%d\") + \"_500m.tif\").ReadAsArray() * 0.3333\n",
    "            kNN_map_avg_valid = kNN_map_avg[lidar_map >= 0.]\n",
    "            lidar_map_valid = lidar_map[lidar_map >= 0.]\n",
    "            rmse = np.sqrt(np.nanmean((kNN_map_avg_valid - lidar_map_valid) ** 2))\n",
    "            rmse_k.append(rmse)\n",
    "        print np.argmin(np.array(rmse_k))\n",
    "        k_list.append(np.argmin(np.array(rmse_k))+1)\n",
    "    print k_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spatial_error_vs_num_of_scenes(random_state):\n",
    "    np.random.seed(random_state)\n",
    "    recon_ts = np.load(\"kNN_training_testing/library_mb_2001_2013.npy\")\n",
    "    recon_fn = pickle.load(open(\"kNN_training_testing/library_mb_2001_2013_filenames.p\", \"rb\"))\n",
    "    lidar_sensor_data = np.load(\"kNN_training_testing/aso_simulated_sensor_data.npy\")\n",
    "    fig, axarr = plt.subplots(ncols=1, nrows=2, figsize=(3.5, 6))\n",
    "    k = 15\n",
    "    random_recon_idx = np.random.randint(0, len(recon_ts), len(recon_ts))\n",
    "    color_code_list = ['m', 'b', 'w', 'c', 'y', 'r', 'g']\n",
    "    date_line_list = []\n",
    "    for j, (sensor, temp_date) in enumerate(zip(lidar_sensor_data, date_list)):\n",
    "        rmse_list = []\n",
    "        mae_list = []\n",
    "        percentage_list = np.arange(0.4, 1.01, 0.01) * 100.\n",
    "        recon_map = gdal.Open(\"MB_recon/\" + temp_date.strftime(\"%d%b%Y\").upper() + \".tif\").ReadAsArray()\n",
    "        lidar_map = gdal.Open(\"ASO_Lidar/MB\"+temp_date.strftime(\"%Y%m%d\") + \"_500m.tif\").ReadAsArray() * 0.3333\n",
    "        recon_map_valid = recon_map[lidar_map >= 0.]\n",
    "        lidar_map_valid = lidar_map[lidar_map >= 0.]\n",
    "        rmse_recon = np.sqrt(np.nanmean((recon_map_valid - lidar_map_valid) ** 2))\n",
    "        mae_recon = np.nanmean(np.absolute(recon_map_valid - lidar_map_valid))\n",
    "        for percentage_scenes in np.arange(0.4, 1.01, 0.01):\n",
    "            temp_end = int(percentage_scenes * float(len(recon_ts)))\n",
    "            temp_recon_idx = random_recon_idx[0:temp_end]\n",
    "            temp_recon_ts = recon_ts[temp_recon_idx]\n",
    "            temp_recon_fn = [recon_fn[i] for i in temp_recon_idx]\n",
    "            kNN = KDTree(temp_recon_ts)\n",
    "            dist, idx = kNN.query(np.array([sensor]), k=k)\n",
    "            temp_fn_list = [temp_recon_fn[i] for i in idx[0]]\n",
    "            kNN_map_sum = 0.\n",
    "            for temp_fn in temp_fn_list:\n",
    "                kNN_map_sum += gdal.Open(temp_fn).ReadAsArray()\n",
    "            kNN_map_avg = kNN_map_sum / float(k)\n",
    "            kNN_map_avg_valid = kNN_map_avg[lidar_map >= 0.]\n",
    "            rmse = np.sqrt(np.nanmean((kNN_map_avg_valid - lidar_map_valid) ** 2))\n",
    "            rmse_list.append(rmse)\n",
    "            mae = np.nanmean(np.absolute(kNN_map_avg_valid - lidar_map_valid))\n",
    "            mae_list.append(mae)\n",
    "        line_1, = axarr[0].plot(percentage_list, rmse_list, '-'+color_code_list[j])\n",
    "        line_2, = axarr[0].plot(percentage_list, np.ones(len(percentage_list))*rmse_recon, '--'+color_code_list[j])\n",
    "        line_3, = axarr[1].plot(percentage_list, mae_list, '-'+color_code_list[j])\n",
    "        line_4, = axarr[1].plot(percentage_list, np.ones(len(percentage_list))*mae_recon, '--'+color_code_list[j])\n",
    "        date_line_list.append(line_1)\n",
    "    axarr[0].legend([line_1, line_2], [\"kNN\", \"Reconstruction\"], frameon=False, prop={'size':8})\n",
    "    axarr[1].legend(date_line_list, date_list, frameon=False, prop={'size':8})\n",
    "    axarr[0].set_ylim([0.05, 0.21])\n",
    "    axarr[1].set_ylim([0.0, 0.13])\n",
    "    axarr[0].set_xlim([40, 100])\n",
    "    axarr[1].set_xlim([40, 100])\n",
    "    axarr[0].yaxis.set_ticks([0.05, 0.1, 0.15, 0.2])\n",
    "    axarr[1].yaxis.set_ticks([0.0, 0.05, 0.10, 0.13])\n",
    "    axarr[0].grid()\n",
    "    axarr[1].grid()\n",
    "    axarr[0].xaxis.set_ticks([40, 70, 100])\n",
    "    axarr[1].xaxis.set_ticks([40, 70, 100])\n",
    "    axarr[1].set_xlabel(\"Percentage of total reconstruction scenes, %\")\n",
    "    axarr[0].set_ylabel(\"Root-mean-square error, m\")\n",
    "    axarr[1].set_ylabel(\"Mean-absolute error, m\")\n",
    "    plt.savefig(\"rmse_mae_percent_scenes_spatial.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the time series of the selected pixels' __snow water equivalent__\n",
    "$$a = b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get colorbar information\n",
    "raster_loc_idx = (np.array([29, 35, 25, 54, 62, 36, 53, 41, 36, 45, 66, 48, 13, 27, 58, 22, 27]), \n",
    "           np.array([29, 15, 42, 52, 50, 41, 40, 34, 17, 20, 42, 58, 44, 51, 18, 35, 63]))\n",
    "dem_array = gdal.Open(\"ASO_Lidar/Merced_500m_DEM.tif\").ReadAsArray()\n",
    "elevation_list = dem_array[raster_loc_idx]\n",
    "print elevation_list\n",
    "elevation_color = (elevation_list - np.min(elevation_list)) / (np.max(elevation_list) - np.min(elevation_list))\n",
    "elevation_gradient = elevation_list[np.argsort(elevation_list)]\n",
    "elevation_color_gradient = elevation_color[np.argsort(elevation_list)]\n",
    "elevation_color_gradient_str = [str(c) for c in elevation_color_gradient]\n",
    "mymap = mpl.colors.LinearSegmentedColormap.from_list('gray',elevation_color_gradient_str)\n",
    "Z = [[0,0],[0,0]]\n",
    "levels = elevation_gradient\n",
    "CS3 = plt.contourf(Z, levels, cmap=mymap)\n",
    "plt.clf()\n",
    "\n",
    "date_list = [date(2014, 3, 24), date(2014, 4, 6), date(2014, 4, 14), date(2014, 4, 23), date(2014, 4, 29), date(2014, 5, 3), date(2014, 5, 12)]\n",
    "weekly_aso_data = np.load(\"aso_simulated_sensor_data.npy\")\n",
    "\n",
    "loc = mdates.WeekdayLocator(byweekday=MO, interval=2)\n",
    "fmt = mdates.DateFormatter('%b %d %Y')\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, 17):\n",
    "    cax = ax.plot(date_list, weekly_aso_data[:, i], '.-', color=str(elevation_color[i]))\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "fig.colorbar(CS3)\n",
    "ax.yaxis.set_ticks([0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raster_loc_idx = pickle.load(open('kNN_training_testing/sensor_idx.p'))\n",
    "dem_array = gdal.Open(\"ASO_Lidar/Merced_500m_DEM.tif\").ReadAsArray()\n",
    "slope_array = gdal.Open(\"ASO_Lidar/Merced_500m_SLP.tif\").ReadAsArray()\n",
    "aspect_array = gdal.Open(\"ASO_Lidar/Merced_500m_ASP.tif\").ReadAsArray()\n",
    "northness_array = gdal.Open(\"ASO_Lidar/Merced_500m_NOR.tif\").ReadAsArray()\n",
    "sensor_elevation = dem_array[raster_loc_idx]\n",
    "dem = dem_array[dem_array >= 1500.]\n",
    "sensor_hist = np.histogram(sensor_elevation, normed=True)\n",
    "basin_hist = np.histogram(dem, normed=True)\n",
    "sensor_cdf = np.cumsum(sensor_hist[0]) / np.cumsum(sensor_hist[0])[-1]\n",
    "sensor_cdf_elev = sensor_hist[1]\n",
    "basin_cdf = np.cumsum(basin_hist[0]) / np.cumsum(sensor_hist[0])[-1]\n",
    "basin_cdf_elev = basin_hist[1]\n",
    "plt.plot(sensor_cdf_elev[:-1] + (sensor_cdf_elev[1] - sensor_cdf_elev[0]) / 2., sensor_cdf, '-g')\n",
    "plt.plot(basin_cdf_elev[:-1] + (basin_cdf_elev[1] - basin_cdf_elev[0]) / 2., basin_cdf, '-b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Process interpolation\n",
    "In the following section, use Gaussian Process to interpolate the SWE across the entire basin at 500-m resolution. The objective is to compare the quality of the reconstruction versus traditional interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_swe(date_obj, schema_name, sensor=False, basin=\"Merced\"):\n",
    "    if type(date_obj)==str:\n",
    "        date_str = date_obj\n",
    "    elif type(date_obj)==date:\n",
    "        date_str = date_obj.strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        print \"The input date_obj dtype is not supported\"\n",
    "        return\n",
    "    if not sensor:\n",
    "        DEM = kNN_db.query_map(\"DEM\", 'topo', basin.lower())\n",
    "        swe = kNN_db.query_map(date_str, schema_name, basin.lower())\n",
    "        data_array = np.column_stack((DEM.flatten(), swe.flatten()))\n",
    "        data_array = data_array[data_array[:, 0] >= 0]\n",
    "        data_array = data_array[:, 1]\n",
    "    else:\n",
    "        data_array = kNN_db.geoms_table_to_map_pixel_values(date_str, 'sensors', 'site_locs', 'site_coords', schema_name, basin.lower())\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train_gp(date_obj, basin='merced'):\n",
    "#     sensor_features = load_features(sensor_feature=True, basin=basin)\n",
    "#     sensor_swe = load_swe(date_obj, 'swe_lidar', sensor=True, basin='merced')\n",
    "#     k1 = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\n",
    "#     k2 = 1.0 * WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "#     kernel = k1 + k2\n",
    "#     gp_obj = GaussianProcessRegressor(kernel=kernel, alpha=0.0001)\n",
    "#     gp_obj.fit(sensor_features, sensor_swe)\n",
    "#     return sensor_features, sensor_swe, gp_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gp(features, swe, k1):\n",
    "    k1 = k1\n",
    "    k2 = 1.0 * WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "    kernel = k1 + k2\n",
    "    gp_obj = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n",
    "    if len(features.shape) == 1:\n",
    "        gp_obj.fit(features[:, np.newaxis], swe)\n",
    "    else:\n",
    "        gp_obj.fit(features, swe)\n",
    "    return gp_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspect_gp(sensor_features, sensor_swe):\n",
    "    for col in range(sensor_features.shape[1]):\n",
    "        temp_feature = sensor_features[:, col]\n",
    "        min_feature = min(temp_feature) - 0.05 * (max(temp_feature) - min(temp_feature))\n",
    "        max_feature = max(temp_feature) + 0.05 * (max(temp_feature) - min(temp_feature))\n",
    "        temp_gp_obj = train_gp(temp_feature, sensor_swe)\n",
    "        X_ = np.linspace(min_feature, max_feature, 100)\n",
    "        y_mean, y_std = temp_gp_obj.predict(X_[:, np.newaxis], return_std=True)\n",
    "        plt.figure()\n",
    "        plt.plot(temp_feature, sensor_swe, '.r', markersize=5)\n",
    "        plt.plot(X_, y_mean, 'k', lw=3, zorder=9)\n",
    "        plt.fill_between(X_, y_mean.flatten() - y_std, y_mean.flatten() + y_std, facecolor='grey', alpha=0.5)\n",
    "        plt.title(str(col+1))\n",
    "        print temp_gp_obj.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inspect():\n",
    "    sensor_features = load_features(sensor_feature=True, basin='merced')\n",
    "    sensor_swe = load_swe('20140324', 'swe_lidar', sensor=True, basin='merced')\n",
    "    inspect_gp(sensor_features, sensor_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gp_analysis(date_str, kernel, basin='merced'):\n",
    "    sensor_features = load_features(sensor_feature=True, basin='merced')\n",
    "    if basin=='merced':\n",
    "        sensor_swe = load_swe(date_str, 'swe_lidar', sensor=True, basin='merced') * 0.333\n",
    "    else:\n",
    "        sensor_swe = load_swe(date_str, 'swe_lidar', sensor=True, basin='merced')\n",
    "    gp_obj = train_gp(sensor_features, sensor_swe, kernel)\n",
    "    spatial_features = load_features(sensor_feature=False, basin='merced')\n",
    "    if basin=='merced':\n",
    "        true_swe = load_swe(date_str, 'swe_lidar', sensor=False, basin='merced') * 0.333\n",
    "    else:\n",
    "        true_swe = load_swe(date_str, 'swe_lidar', sensor=False, basin='merced')\n",
    "    pred_swe_mean, pred_swe_std = gp_obj.predict(spatial_features, return_std=True)\n",
    "    pred_swe_mean = pred_swe_mean[true_swe > 0]\n",
    "    true_swe = true_swe[true_swe > 0]\n",
    "    return true_swe, pred_swe_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scatter_plot_comparison(date_list, true_swe_list, pred_swe_list, save_fn = None):\n",
    "    nrows = len(date_list)\n",
    "    fig, axarr = plt.subplots(ncols=1, nrows=nrows, figsize=(3.5, 20))\n",
    "    for j, temp_date in enumerate(date_list):\n",
    "        true_swe = true_swe_list[j]\n",
    "        pred_swe = pred_swe_list[j]\n",
    "        \n",
    "        slope, intercept, r_value, p_value, std_err = linregress(true_swe,pred_swe.flatten())\n",
    "        x_0 = [0., 1.]\n",
    "        y_0 = [intercept, slope+intercept]\n",
    "        print x_0, y_0\n",
    "        \n",
    "        \n",
    "        axarr[j].plot(true_swe, pred_swe.flatten(), '.k', markersize=5)\n",
    "        axarr[j].plot([0, 1], [0, 1], '--k')\n",
    "        axarr[j].plot(x_0, y_0, '-b')\n",
    "        axarr[j].set_xlim([0, 1])\n",
    "        axarr[j].set_ylim([0, 1])\n",
    "        \n",
    "        if j < nrows:\n",
    "            axarr[j].xaxis.set_ticklabels([])\n",
    "            axarr[j].yaxis.set_ticklabels([\"\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"])\n",
    "        if j==nrows:\n",
    "            axarr[j].xaxis.set_ticklabels([\"\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"])\n",
    "        axarr[j].text(0.55, 0.1, temp_date.strftime(\"%Y-%m-%d\"), fontsize=8)\n",
    "    fig.text(0.25, 0.09, 'Lidar measured SWE, m')\n",
    "    fig.text(0.0, 0.6, 'kNN interpolated SWE, m', rotation=90)\n",
    "    plt.subplots_adjust(left=0.14, wspace=0, hspace=0)\n",
    "    if save_fn:\n",
    "        plt.savefig(save_fn)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernels = [0.5 * RBF(length_scale=100.0, length_scale_bounds=(1e-1, 2000.0)), \n",
    "           0.5 * Matern(length_scale=100.0, length_scale_bounds=(1e-1, 4000.0)), \n",
    "           1.0 * DotProduct(sigma_0=100.0, sigma_0_bounds=(1e-02, 1000.0)),\n",
    "           1.0 * WhiteKernel()]\n",
    "best_kernel = kernels[2] + kernels[-1]\n",
    "alternative_kernel = sum(kernels)\n",
    "fns = os.listdir(\"ASO_Lidar/\")\n",
    "fns = [fn for fn in fns if fn.startswith(\"MB\")]\n",
    "date_str_list = sorted([fn[2:10] for fn in fns])\n",
    "print date_str_list\n",
    "date_list = [datetime.strptime(date_str, '%Y%m%d') for date_str in date_str_list]\n",
    "mae_k1_list = []\n",
    "mae_k2_list = []\n",
    "rmse_k1_list = []\n",
    "rmse_k2_list = []\n",
    "true_swe_list = []\n",
    "pred_swe_list = []\n",
    "for date_str in date_str_list:\n",
    "    true_swe_k1, pred_swe_k1 = gp_analysis(date_str, best_kernel)\n",
    "    true_swe_k2, pred_swe_k2 = gp_analysis(date_str, alternative_kernel)\n",
    "    true_swe_list.append(true_swe_k2)\n",
    "    pred_swe_list.append(pred_swe_k2)\n",
    "    mae_k1 = mean_absolute_error(true_swe_k1, pred_swe_k1)\n",
    "    rmse_k1 = np.sqrt(mean_squared_error(true_swe_k1, pred_swe_k1))\n",
    "    mae_k2 = mean_absolute_error(true_swe_k2, pred_swe_k2)\n",
    "    rmse_k2 = np.sqrt(mean_squared_error(true_swe_k2, pred_swe_k2))\n",
    "    mae_k1_list.append(mae_k1)\n",
    "    mae_k2_list.append(mae_k2)\n",
    "    rmse_k1_list.append(rmse_k1)\n",
    "    rmse_k2_list.append(rmse_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(ncols=1, nrows=2)\n",
    "loc = mdates.WeekdayLocator(byweekday=MO, interval=2)\n",
    "fmt = mdates.DateFormatter('%b %d %Y')\n",
    "axarr[0].plot(date_list, rmse_k2_list, '-k')\n",
    "axarr[0].plot(date_list, rmse_k1_list, '--k')\n",
    "axarr[1].plot(date_list, mae_k2_list, '-k')\n",
    "axarr[1].plot(date_list, mae_k1_list, '--k')\n",
    "axarr[0].xaxis.set_major_locator(loc)\n",
    "axarr[0].xaxis.set_major_formatter(fmt)\n",
    "axarr[1].xaxis.set_major_locator(loc)\n",
    "axarr[1].xaxis.set_major_formatter(fmt)\n",
    "axarr[0].grid()\n",
    "axarr[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try kNN + GP, will GP improve kNN performance? Good question suggested by Sami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kNN_resGP():\n",
    "    recon_ts = np.load(\"kNN_training_testing/library_mb_2001_2013.npy\")\n",
    "    recon_fn = pickle.load(open(\"kNN_training_testing/library_mb_2001_2013_filenames.p\", \"rb\"))\n",
    "    lidar_sensor_data = np.load(\"kNN_training_testing/aso_simulated_sensor_data.npy\")\n",
    "    res_GP_sensor_features = load_features(sensor_feature=True, basin='Merced')\n",
    "    res_GP_basin_features = load_features(sensor_feature=False, exclude_null=False, basin='Merced')\n",
    "    GP_kernels = kernels = [0.5 * RBF(length_scale=100.0, length_scale_bounds=(1e-1, 2000.0)), \n",
    "           0.5 * Matern(length_scale=100.0, length_scale_bounds=(1e-1, 4000.0)), \n",
    "           1.0 * DotProduct(sigma_0=100.0, sigma_0_bounds=(1e-02, 1000.0)),\n",
    "           1.0 * WhiteKernel()]\n",
    "    GP_kernel = sum(GP_kernels)\n",
    "    emp_cov = EmpiricalCovariance().fit(recon_ts)\n",
    "    emp_cov_matrix = emp_cov.get_precision()\n",
    "    dist = DistanceMetric.get_metric('mahalanobis', V=emp_cov_matrix)\n",
    "    kNN = BallTree(recon_ts, metric=dist)\n",
    "    k = 30\n",
    "    true_swe_list = []\n",
    "    pred_swe_list = []\n",
    "#     fig, axarr = plt.subplots(ncols=3, nrows=7, figsize=(6, 10))\n",
    "    for j, (sensor, temp_date) in enumerate(zip(lidar_sensor_data, date_list)):\n",
    "        dist, idx = kNN.query(np.array([sensor]), k=k)\n",
    "        temp_fn_list = [recon_fn[i] for i in idx[0]]\n",
    "        kNN_map_sum = 0.\n",
    "        for temp_fn in temp_fn_list:\n",
    "            kNN_map_sum += gdal.Open(temp_fn).ReadAsArray()\n",
    "        kNN_map_avg = kNN_map_sum / float(k)\n",
    "        kNN_sensor_predict = np.average(recon_ts[idx[0]], axis=0)\n",
    "        kNN_res = sensor - kNN_sensor_predict\n",
    "        kNN_res_GP = train_gp(res_GP_sensor_features, kNN_res, GP_kernel)\n",
    "        kNN_res_GP_mean, kNN_res_GP_std = kNN_res_GP.predict(res_GP_basin_features, return_std=True)\n",
    "        kNN_map_avg += np.reshape(kNN_res_GP_mean, kNN_map_avg.shape)\n",
    "        lidar_map = gdal.Open(\"ASO_Lidar/MB\"+temp_date.strftime(\"%Y%m%d\") + \"_500m.tif\").ReadAsArray() * 0.3333\n",
    "        kNN_map_avg = kNN_map_avg.flatten()\n",
    "        lidar_map = lidar_map.flatten()\n",
    "        kNN_map_avg_valid = kNN_map_avg[res_GP_basin_features[:, 2]>=0]\n",
    "        lidar_map_valid = lidar_map[res_GP_basin_features[:, 2]>=0]\n",
    "        kNN_map_avg_valid = kNN_map_avg_valid[lidar_map_valid>=0]\n",
    "        lidar_map_valid = lidar_map_valid[lidar_map_valid>=0]\n",
    "        true_swe_list.append(lidar_map_valid)\n",
    "        pred_swe_list.append(kNN_map_avg_valid)\n",
    "    scatter_plot_comparison(date_list, true_swe_list, pred_swe_list)\n",
    "    return true_swe_list, pred_swe_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try kNN plus Neural Net, will that working better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_NeuralNet(X_train, Y_train, X_test):\n",
    "    \n",
    "    # initialize variable functions\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    # initialize variable functions\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    # Define tensors of the input and output data\n",
    "    r_x = tf.placeholder(tf.float32, shape=[None, 6])\n",
    "    r_y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # configure parameters of the Neural network\n",
    "    total_len = X_train.shape[0]\n",
    "\n",
    "    # Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_epochs = 2000\n",
    "    batch_size = 5\n",
    "    display_step = 500\n",
    "    dropout_rate = 0.6\n",
    "    \n",
    "    # Network Parameters\n",
    "    n_hidden_1 = 12 # 1st layer number of features\n",
    "    n_hidden_2 = 72 # 2nd layer number of features\n",
    "    n_hidden_3 = 72\n",
    "    n_hidden_4 = 128\n",
    "    n_input = X_train.shape[1]\n",
    "    n_classes = 1\n",
    "    \n",
    "    # Configure my networks\n",
    "    def multilayer_perceptron(x, keep_prob, weights, biases):\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "        layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "        # Drop layer\n",
    "        drop_layer = tf.nn.dropout(layer_4, keep_prob)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(drop_layer, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "    \n",
    "    weights = {\n",
    "        'h1': weight_variable([n_input, n_hidden_1]),\n",
    "        'h2': weight_variable([n_hidden_1, n_hidden_2]),\n",
    "        'h3': weight_variable([n_hidden_2, n_hidden_3]),\n",
    "        'h4': weight_variable([n_hidden_3, n_hidden_4]),\n",
    "        'out': weight_variable([n_hidden_4, n_classes])\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': bias_variable([n_hidden_1]),\n",
    "        'b2': bias_variable([n_hidden_2]),\n",
    "        'b3': bias_variable([n_hidden_3]),\n",
    "        'b4': bias_variable([n_hidden_4]),\n",
    "        'out': bias_variable([n_classes])\n",
    "    }\n",
    "\n",
    "    # Define prediction output tensor, cost tensor, and the optimizer\n",
    "    r_y = multilayer_perceptron(r_x, keep_prob, weights, biases)\n",
    "    cost = tf.reduce_mean(tf.square(r_y-r_y_))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Train the networks with batches\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(total_len/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch+1):\n",
    "                if i != total_batch:\n",
    "                    batch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "                    batch_y = Y_train[i*batch_size:(i+1)*batch_size]\n",
    "                else:\n",
    "                    batch_x = X_train[i*batch_size:]\n",
    "                    batch_y = Y_train[i*batch_size:]\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                _, c, p = sess.run([optimizer, cost, r_y], feed_dict={r_x: batch_x, r_y_: batch_y[:, np.newaxis], \n",
    "                                                                      keep_prob: dropout_rate})\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            # sample prediction\n",
    "            label_value = batch_y\n",
    "            estimate = p\n",
    "            err = label_value-estimate\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost))\n",
    "                print (\"[*]----------------------------\")\n",
    "                for i in xrange(3):\n",
    "                    print (\"label value:\", label_value[i], \\\n",
    "                        \"estimated value:\", estimate[i])\n",
    "                print (\"[*]============================\")\n",
    "\n",
    "        print (\"Optimization Finished!\")\n",
    "        p = sess.run(r_y, feed_dict={r_x: X_test, keep_prob:1.0})\n",
    "    # return the prediction of the test x\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kNN_resNN():\n",
    "    recon_ts = np.load(\"kNN_training_testing/library_mb_2001_2013.npy\")\n",
    "    recon_fn = pickle.load(open(\"kNN_training_testing/library_mb_2001_2013_filenames.p\", \"rb\"))\n",
    "    lidar_sensor_data = np.load(\"kNN_training_testing/aso_simulated_sensor_data.npy\")\n",
    "    res_NN_sensor_features = load_features(sensor_feature=True, basin='Merced')\n",
    "    res_NN_basin_features = load_features(sensor_feature=False, exclude_null=False, basin='Merced')\n",
    "    emp_cov = EmpiricalCovariance().fit(recon_ts)\n",
    "    emp_cov_matrix = emp_cov.get_precision()\n",
    "    dist = DistanceMetric.get_metric('mahalanobis', V=emp_cov_matrix)\n",
    "    kNN = BallTree(recon_ts, metric=dist)\n",
    "    k = 30\n",
    "    true_swe_list = []\n",
    "    pred_swe_list = []\n",
    "#     fig, axarr = plt.subplots(ncols=3, nrows=7, figsize=(6, 10))\n",
    "    for j, (sensor, temp_date) in enumerate(zip(lidar_sensor_data, date_list)):\n",
    "        # Query the k-nearest neighbors from the BallTree object\n",
    "        dist, idx = kNN.query(np.array([sensor]), k=k)\n",
    "        temp_fn_list = [recon_fn[i] for i in idx[0]]\n",
    "        \n",
    "        # Average all the maps that inside the nearest neighbors\n",
    "        kNN_map_sum = 0.\n",
    "        for temp_fn in temp_fn_list:\n",
    "            kNN_map_sum += gdal.Open(temp_fn).ReadAsArray()\n",
    "        kNN_map_avg = kNN_map_sum / float(k)\n",
    "        kNN_sensor_predict = np.average(recon_ts[idx[0]], axis=0)\n",
    "        \n",
    "        # Calculate the residual of the kNN regression results\n",
    "        kNN_res = sensor - kNN_sensor_predict\n",
    "        kNN_res_NN_pred = res_NeuralNet(res_NN_sensor_features, kNN_res, res_NN_basin_features)\n",
    "        \n",
    "        # Calculate the final prediction: kNN + Neural Network residual interpolation\n",
    "        kNN_map_avg += np.reshape(kNN_res_NN_pred, kNN_map_avg.shape)\n",
    "        \n",
    "        # Format the pred and true y\n",
    "        lidar_map = gdal.Open(\"ASO_Lidar/MB\"+temp_date.strftime(\"%Y%m%d\") + \"_500m.tif\").ReadAsArray() * 0.3333\n",
    "        kNN_map_avg = kNN_map_avg.flatten()\n",
    "        lidar_map = lidar_map.flatten()\n",
    "        kNN_map_avg_valid = kNN_map_avg[res_NN_basin_features[:, 2]>=0]\n",
    "        lidar_map_valid = lidar_map[res_NN_basin_features[:, 2]>=0]\n",
    "        kNN_map_avg_valid = kNN_map_avg_valid[lidar_map_valid>=0]\n",
    "        lidar_map_valid = lidar_map_valid[lidar_map_valid>=0]\n",
    "        true_swe_list.append(lidar_map_valid)\n",
    "        pred_swe_list.append(kNN_map_avg_valid)\n",
    "        \n",
    "    scatter_plot_comparison(date_list, true_swe_list, pred_swe_list, \"knn_withNN_scatter.pdf\")\n",
    "    return true_swe_list, pred_swe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_swe_list, pred_swe_list = kNN_resGP()\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "for i, date_str in enumerate(date_str_list):\n",
    "    true_swe, pred_swe = true_swe_list[i], pred_swe_list[i]\n",
    "    mae = mean_absolute_error(true_swe, pred_swe)\n",
    "    rmse = np.sqrt(mean_squared_error(true_swe, pred_swe))\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    print np.mean(true_swe)\n",
    "# fig, axarr = plt.subplots(ncols=1, nrows=2, figsize=(5, 5), sharex=True)\n",
    "# loc = mdates.WeekdayLocator(byweekday=MO, interval=2)\n",
    "# fmt = mdates.DateFormatter('%b %d %Y')\n",
    "# axarr[0].plot(date_list, rmse_list, '-k')\n",
    "# axarr[1].plot(date_list, mae_list, '-k')\n",
    "# axarr[0].xaxis.set_major_locator(loc)\n",
    "# axarr[0].xaxis.set_major_formatter(fmt)\n",
    "# axarr[1].xaxis.set_major_locator(loc)\n",
    "# axarr[1].xaxis.set_major_formatter(fmt)\n",
    "# axarr[0].grid()\n",
    "# axarr[1].grid()\n",
    "# axarr[0].set_ylabel(\"RMSE, m\")\n",
    "# axarr[1].set_ylabel(\"MAE, m\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
